{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheRealFREDP3D/Awesome-LLM4Cybersecurity/blob/main/Ollama_ColabV4_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore the Advanced Ollama-Companion:\n",
        "# Streamlit Enhanced\n",
        "---  \n",
        "\n",
        "\n",
        "**Use this command to install local**:  \n",
        "\n",
        "\n",
        "```\n",
        "curl https://raw.githubusercontent.com/Luxadevi/Ollama-Companion/main/install.sh | sh  \n",
        "```\n",
        "\n",
        "  \n",
        "Welcome to the latest version of Ollama-Companion, now enhanced with Streamlit for a more interactive and intuitive user experience. As the developer behind this innovative project, I'm excited to introduce a suite of new features that redefine how you interact with and manage language models.\n",
        "\n",
        "Key Innovations in Ollama-Companion:\n",
        "1. Quantization of Huggingface Models via UI\n",
        "The foremost feature of Ollama-Companion is the ability to quantize Huggingface models through a user-friendly interface. This functionality allows you to efficiently convert models into different formats, catering to a variety of computational needs.\n",
        "\n",
        "2. Dynamic Module Integration\n",
        "Seamlessly integrate a variety of modules as defined in shared.py, ensuring a modular and scalable approach to application development.\n",
        "\n",
        "3. Streamlit-Powered User Interface\n",
        "The UI has been revamped using Streamlit to enhance intuitiveness and responsiveness, making it easier to navigate and interact with various features.\n",
        "\n",
        "4. Enhanced Model Interaction Features\n",
        "Modelfile Manager: Beyond model selection, this feature lets you delve into the details of each model, providing options to view, manage, and even delete model files as required.\n",
        "Interactive Modelfile Creator: Customize your model files in real-time, offering enhanced control and flexibility.\n",
        "5. Chat Interface with LLAVA Image Analysis\n",
        "The chat interface is equipped with LLAVA for image recognition and analysis, adding a dynamic and interactive dimension to your conversations with language models.\n",
        "\n",
        "6. Advanced Configuration Tools\n",
        "Ollama API Configurator: Manage Ollama API endpoints directly from the UI.\n",
        "LiteLLM Proxy and Public Endpoint: Easily set up proxies and public endpoints, ensuring secure and efficient model sharing.\n",
        "7. Efficient Model Management Systems\n",
        "Fast Model Downloading: Download models from Huggingface with improved speed.\n",
        "Quantization Options: Choose between high or medium precision GGUF formats for model transformation.\n",
        "Secure Model Uploads to Huggingface: Upload models to Huggingface confidently with enhanced security measures.\n",
        "8. Security Enhancements\n",
        "Token Encryption: Protect your Huggingface token with an additional layer of encryption for increased data security.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SIujJTVa7hAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to run\n",
        "---\n",
        "* Run the cells below by clicking the play button to download  and start ollama with a public url .  \n",
        "\n"
      ],
      "metadata": {
        "id": "3gQuQak3Ckh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/Luxadevi/Ollama-Companion/main/install.sh  && sudo chmod +x install.sh 2>&1 /dev/null\n",
        "#Sets up Latest version of Llama.cpp for quanting.\n",
        "#If You need to build a new one use the argument `-colab_compile`\n",
        "#Also interactive installer available use -interactive or -i\n",
        "#Using a virtual enviroment is not recommended within Google_Colab\n",
        "#Docs are embedded within Ollama-Companion\n",
        "!/content/install.sh -colab"
      ],
      "metadata": {
        "id": "gQSkK1w82y52",
        "outputId": "23cc6414-e66b-4ba7-afb8-033998b78c5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proceeding with colab installation.\n",
            "Starting Colab installation...\n",
            "This uses pre-compiled llama.cpp binaries.\n",
            "To freshly compile a new version, use -colab_compile.\n",
            "Refer to the llama.cpp GitHub repository for more info.\n",
            "Installing required packages...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If stopped or wanting to relaunch.**   \n",
        "Run this cell to restart your LLM stack"
      ],
      "metadata": {
        "id": "9QADK7qltjVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/Ollama-Companion/start.sh"
      ],
      "metadata": {
        "id": "aCwfO2OT6W_8",
        "outputId": "4e2cece3-e605-4567-81e2-e6c80bba9f97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Terminated existing Ollama processes\n",
            "Starting Ollama-Companion with a public URL\n",
            "Starting Cloudflare Tunnel...\n",
            "Tunnel URL: https://produces-offline-bear-girl.trycloudflare.com\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.127.8.65:8501\u001b[0m\n",
            "\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}